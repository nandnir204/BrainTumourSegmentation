{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10672893,"sourceType":"datasetVersion","datasetId":6610755},{"sourceId":11136316,"sourceType":"datasetVersion","datasetId":6945926}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation-models-pytorch opencv-python ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport cv2\nimport segmentation_models_pytorch as smp\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_score(pred, target, epsilon=1e-6):\n    pred = (pred > 0.5).float()\n    intersection = (pred * target).sum()\n    return (2. * intersection + epsilon) / (pred.sum() + target.sum() + epsilon)\n\ndef iou_score(pred, target, epsilon=1e-6):\n    pred = (pred > 0.5).float()\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    return (intersection + epsilon) / (union + epsilon)\n\ndef precision_recall_f1(pred, target, epsilon=1e-6):\n    pred = (pred > 0.5).float()\n    tp = (pred * target).sum()\n    fp = (pred * (1 - target)).sum()\n    fn = ((1 - pred) * target).sum()\n    \n    precision = tp / (tp + fp + epsilon)\n    recall = tp / (tp + fn + epsilon)\n    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n    \n    return precision, recall, f1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_metrics(pred, target):\n    dice = dice_score(pred, target)\n    iou = iou_score(pred, target)\n    precision, recall, f1 = precision_recall_f1(pred, target)\n    return {\n        'Dice Score': dice.item(),\n        'IoU Score': iou.item(),\n        'Precision': precision.item(),\n        'Recall': recall.item(),\n        'F1-Score': f1.item()\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_metric_lists(metrics_list):\n    dice_scores, iou_scores, precisions, recalls, f1_scores = [], [], [], [], []\n    for epoch_metrics in metrics_list:\n        dice_scores.append(epoch_metrics['Dice Score'])\n        iou_scores.append(epoch_metrics['IoU Score'])\n        precisions.append(epoch_metrics['Precision'])\n        recalls.append(epoch_metrics['Recall'])\n        f1_scores.append(epoch_metrics['F1-Score'])\n    return dice_scores, iou_scores, precisions, recalls, f1_scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plotFunction(num_epochs, losses, dice_scores, iou_scores, precisions, recalls, f1_scores, graph_names):\n    \n    # Prepare epochs range\n    epochs = range(1, num_epochs + 1)\n    \n    # Set figure size\n    plt.figure(figsize=(16, 10))\n    \n    # Plot losses vs. epochs\n    plt.subplot(3, 3, 1)\n    plt.plot(epochs, losses, label='Loss', color=\"blue\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss vs. Epochs\")\n    plt.legend()\n\n    # Plot accuracies vs. epochs\n    plt.subplot(3, 3, 2)\n    plt.plot(epochs, dice_scores, label='dice_scores', color=\"orange\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"dice_score\")\n    plt.title(\"dice_scores vs. Epochs\")\n    plt.legend()\n\n    # Plot recalls vs. epochs\n    plt.subplot(3, 3, 3)\n    plt.plot(epochs, iou_scores, label='iou_scores', color=\"purple\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"iou_score\")\n    plt.title(\"iou_scores vs. Epochs\")\n    plt.legend()\n\n    # Plot F1-scores vs. epochs\n    plt.subplot(3, 3, 4)\n    plt.plot(epochs, precisions, label='precisions', color=\"red\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"precisions\")\n    plt.title(\"precisions vs. Epochs\")\n    plt.legend()\n\n    # Plot precisions vs. epochs\n    plt.subplot(3, 3, 5)\n    plt.plot(epochs, recalls, label='recalls', color=\"blue\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"recalls\")\n    plt.title(\"recalls vs. Epochs\")\n    plt.legend()\n\n    # Plot ROC-AUC vs. epochs\n    plt.subplot(3, 3, 6)\n    plt.plot(epochs, f1_scores, label='f1_scores', color=\"green\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"f1_scores\")\n    plt.title(\"f1_scores vs. Epochs\")\n    plt.legend()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save the entire figure (all subplots) as an image\n    plt.savefig('/kaggle/working/' + graph_names + '.png')  # Save as PNG file\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef save_checkpoint(\n    epoch,\n    model,\n    optimizer,\n    train_metrics,\n    val_metrics,\n    save_dir,\n    save_filename,\n    is_best\n):\n    \"\"\"\n    Save the model checkpoint with metrics for all epochs and model weights for the last epoch.\n\n    Args:\n        epoch (int): Current epoch number.\n        model (torch.nn.Module): The PyTorch model.\n        optimizer (torch.optim.Optimizer): The optimizer used for training.\n        train_metrics_all_epochs (list): List of training metrics for all epochs.\n        val_metrics_all_epochs (list): List of validation metrics for all epochs.\n        save_dir (str): Directory to save the checkpoint.\n        save_filename (str): Base filename for the checkpoint.\n\n    Returns:\n        None\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Save metrics for all epochs in a JSON file (append mode)\n    metrics = {\n        'epoch': epoch,\n        'train_metrics': train_metrics,\n        'val_metrics': val_metrics,\n    }\n    \n    metrics_file_path = os.path.join(save_dir, f\"{save_filename}_metrics.json\")\n    # Check if the file exists and has content\n    if os.path.exists(metrics_file_path):\n        with open(metrics_file_path, 'r') as f:\n            try:\n                all_metrics = json.load(f)  # Attempt to load existing metrics\n            except json.JSONDecodeError:\n                all_metrics = []  # If the file is empty or corrupted, initialize as empty list\n    else:\n        all_metrics = []  # If the file doesn't exist, initialize as empty list\n    \n    all_metrics.append(metrics)\n    \n    with open(metrics_file_path, 'w') as f:\n            json.dump(all_metrics, f, indent=4)\n    print(f\"Metrics for epoch {epoch} saved at: {metrics_file_path}\")\n\n    \n    # Save model weights and optimizer state for the last epoch only\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n    }\n    checkpoint_file_path_last = os.path.join(save_dir, \"last_.pt\")\n    torch.save(checkpoint, checkpoint_file_path_last)\n    # if xm.is_master_ordinal():\n        # torch.save(checkpoint, checkpoint_file_path)\n\n    print(f\"Model and optimizer state for the last epoch saved at: {checkpoint_file_path_last}\")\n\n    if(is_best == True):\n        checkpoint_file_path_best = os.path.join(save_dir, \"best_.pt\")\n        torch.save(checkpoint, checkpoint_file_path_best)\n        # if xm.is_master_ordinal():\n            # torch.save(checkpoint, checkpoint_file_path)\n\n        print(f\"Model and optimizer state for the best epoch saved at: {checkpoint_file_path_best}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    train_metrics = []\n    total_loss = 0.0\n    \n    for images, masks in tqdm(dataloader, desc=\"Training\", leave=True):\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        metrics = calculate_metrics(outputs, masks)\n        train_metrics.append(metrics)\n    \n    avg_loss = total_loss / len(dataloader)\n    avg_metrics = {key: sum(d[key] for d in train_metrics) / len(train_metrics) for key in train_metrics[0]}\n    return avg_loss, avg_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    val_metrics = []\n    total_loss = 0.0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Validation\", leave=True):\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            total_loss += loss.item()\n            metrics = calculate_metrics(outputs, masks)\n            val_metrics.append(metrics)\n    \n    avg_loss = total_loss / len(dataloader)\n    avg_metrics = {key: sum(d[key] for d in val_metrics) / len(val_metrics) for key in val_metrics[0]}\n    return avg_loss, avg_metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs, save_dir, save_filename):\n    train_losses, val_losses = [], []\n    train_metrics_list, val_metrics_list = [], []\n    global global_val_dice\n    global_val_dice = 0.0\n    \n    os.makedirs(save_dir, exist_ok=True)\n\n    \n    for epoch in range(epochs):\n        print(\"Active and training is going on...\")\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        \n        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, device)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_metrics_list.append(train_metrics)\n        val_metrics_list.append(val_metrics)\n        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Metrics: {train_metrics}\")\n        print(f\"Val Loss: {val_loss:.4f}, Metrics: {val_metrics}\\n\")\n\n        \n\n        if(val_metrics[\"Dice Score\"] >= global_val_dice):\n            save_checkpoint(epoch, model, optimizer, train_metrics, val_metrics, save_dir, save_filename, True)\n            global_val_dice = val_metrics[\"Dice Score\"]\n        else:\n            save_checkpoint(epoch, model, optimizer, train_metrics, val_metrics, save_dir, save_filename, False)\n\n\n    \n    train_dice, train_iou, train_precision, train_recall, train_f1 = extract_metric_lists(train_metrics_list)\n    val_dice, val_iou, val_precision, val_recall, val_f1 = extract_metric_lists(val_metrics_list)\n\n    plotFunction(epochs, train_losses, train_dice, train_iou, train_precision, train_recall, train_f1, \"Training graphs\")\n    plotFunction(epochs, val_losses, val_dice, val_iou, val_precision, val_recall, val_f1, \"validation graphs\")\n    \n    return ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Dataset (assuming your dataset returns images and multilabel targets)\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None, mask_transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n        self.mask_transform = mask_transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        \n        if image.mode == 'L':\n            image = image.convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n         # Ensure mask is numeric\n        mask = Image.open(self.mask_paths[idx])\n        # if isinstance(mask, str):  # Convert if it's a string\n        #     mask = np.array(Image.open(mask))  # Load mask as NumPy array\n        \n        if mask_transform:\n            mask = self.mask_transform(mask)\n    \n        # masks = torch.tensor(self.masks[idx], dtype=torch.float32)\n        return image, mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((640, 640)),  # Resizing to 640x640\n    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n    transforms.ToTensor(),  # Convert image to Tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize((640, 640)),  # Resizing to 640x640\n    transforms.ToTensor(),  # Convert image to Tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n])\n\nmask_transform = transforms.Compose([\n            transforms.Resize((640, 640)),  # Resize mask to match model output\n            transforms.ToTensor()          # Convert to tensor (scales to [0,1])\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\npath = '/kaggle/input/figsharedataset/images/'\nImages = os.listdir('/kaggle/input/figsharedataset/images/')\nMasks = os.listdir('/kaggle/input/figsharedataset/masks/')\nlen(Images)\nlen(Masks)\n\ncount = 0\n# gray_scale_images = [image = Image.open(image) for image in Images if image.mode != 'RGB']\nfor image in Images:\n    image = Image.open(os.path.join(path, image))\n    if image.mode != 'RGB':\n        count+=1\n\nprint(f\"total number of images in Images dataset = {len(Images)}\")\nprint(f\"total number of gray scale images in Images dataset = {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(os.path.join(path, Images[0]))\nprint(f\"mode of image is = {image.mode}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Images = os.listdir('/kaggle/input/figsharedataset/images/')\nMasks = os.listdir('/kaggle/input/figsharedataset/masks/')\n\nbase_image_path = '/kaggle/input/figsharedataset/images'\nbase_masks_path = '/kaggle/input/figsharedataset/masks'\n\nimage_paths = [os.path.join(base_image_path, image_name) for image_name in Images]\nmask_paths = [os.path.join(base_masks_path, mask_name) for mask_name in Masks]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n# sample = Image.open(image_paths[0])\nsample = cv2.imread(image_paths[0])\nprint(sample.shape)\n# print(sample.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nimages_train_paths, images_test_paths, masks_train_paths, masks_test_paths = train_test_split(image_paths, mask_paths, \n                                   random_state=104,  \n                                   test_size=0.20,  \n                                   shuffle=True) \n\nimages_train_paths, images_val_paths, masks_train_paths, masks_val_paths = train_test_split(images_train_paths, masks_train_paths, \n                                   random_state=104,  \n                                   test_size=0.10,  \n                                   shuffle=True) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load pretrained U-Net model\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",  # Backbone model\n    encoder_weights=\"imagenet\",  # Use pretrained weights\n    in_channels=3,  # 3-channel RGB input\n    classes=1 # Binary segmentation\n    # activation=None,  # Use sigmoid during inference\n)\n\n\n# Initialize and move the model to the correct device\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\n# model.load_state_dict(checkpoint['model_state_dict'])\n# Summarize the model\nsummary(model, input_size=(1, 3, 640, 640), col_names=[\"input_size\", \"output_size\", \"num_params\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\n# Define scheduler - this will decrease the learning rate by 0.5 after 3 successive epochs with no improvement in validation loss\n# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = '/kaggle/working/segmentation'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nsave_filename = \"checkpoints\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your dataset\ntrain_dataset = CustomDataset(images_train_paths, masks_train_paths, transform=transform_train, mask_transform=mask_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n\nval_dataset = CustomDataset(images_val_paths, masks_val_paths, transform=transform_val, mask_transform=mask_transform)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 50\ntrain_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs, save_dir=save_dir, save_filename=save_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"import random \n\ndef evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    test_metrics = []\n    total_loss = 0.0\n    sample_images, sample_masks, sample_preds = [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Testing\", leave=True):\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n           \n            metrics = calculate_metrics(outputs, masks)\n            test_metrics.append(metrics)\n\n            # Select 5 random images for visualization\n            if len(sample_images) < 5:\n                idx = random.randint(0, images.shape[0] - 1)  # Choose a random sample from the batch\n                sample_images.append(images[idx].cpu())\n                sample_masks.append(masks[idx].cpu())\n                sample_preds.append(outputs[idx].sigmoid().cpu())  # Convert logits to probabilities\n\n    # avg_loss = total_loss / len(dataloader)\n    avg_metrics = {key: sum(d[key] for d in test_metrics) / len(test_metrics) for key in test_metrics[0]}\n    return avg_metrics, sample_images, sample_masks, sample_preds #, avg_loss\n                      ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your dataset\ntest_dataset = CustomDataset(images_test_paths, masks_test_paths, transform=transform_val, mask_transform=mask_transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics_list = []\ntest_metrics, sample_images, sample_masks, sample_preds  = evaluate_model(model, test_loader, criterion, device)\n\ntest_metrics_list.append(test_metrics)                      \ntest_dice, test_iou, test_precision, test_recall, test_f1 = extract_metric_lists(test_metrics_list)    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test Metrics :- \")\nprint(f\"Dice score = {test_dice[0]*100 : .4f}\")\nprint(f\"IOU score = {test_iou[0]*100 : .4f}\")\nprint(f\"precision = {test_precision[0]*100 : .4f}\")\nprint(f\"recall = {test_recall[0]*100 : .4f}\")\nprint(f\"f1-score = {test_f1[0]*100 : .4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef visualize_predictions(sample_images, sample_masks, sample_preds, segmentation_images, num_images=5):\n    num_samples = min(len(sample_images), num_images)\n    indices = random.sample(range(len(sample_images)), num_samples)\n    \n    plt.figure(figsize=(12, 4 * num_samples))\n    \n    for i, idx in enumerate(indices):\n        plt.subplot(num_samples, 3, 3*i + 1)\n        plt.imshow(sample_images[idx].permute(1, 2, 0).numpy())\n        plt.title(\"Original Image\")\n        plt.axis(\"off\")\n        \n        plt.subplot(num_samples, 3, 3*i + 2)\n        plt.imshow(sample_masks[idx].numpy().squeeze(), cmap=\"gray\")\n        plt.title(\"Ground Truth Mask\")\n        plt.axis(\"off\")\n        \n        plt.subplot(num_samples, 3, 3*i + 3)\n        predicted_mask = (sample_preds[idx] > 0.5).numpy().squeeze()  # Thresholding\n        plt.imshow(predicted_mask, cmap=\"gray\")\n        plt.title(\"Predicted Mask\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n   \n    plt.savefig('/kaggle/working/' + segmentation_images + '.png')  # Save as PNG file\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize some predictions\nnum_images = 10\nimage_filename = \"segmentation_images\"\nvisualize_predictions(sample_images, sample_masks, sample_preds, image_filename, num_images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# saving model","metadata":{}},{"cell_type":"code","source":"# Remove the destination directory if it exists (force deletion)\n!rm -rf /kaggle/working/brainTumourSegmentation_model\n\n# Create the destination directory\n!mkdir /kaggle/working/brainTumourSegmentation_model\n\n# Move the folder to the new directory\n!mv /kaggle/working/segmentation /kaggle/working/brainTumourSegmentation_model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!cp /kaggle/input/kaggle-json/kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Fetch the Kaggle username dynamically (if configured) or replace with your username\nwith open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"r\") as f:\n    kaggle_config = json.load(f)\n\nkaggle_username = kaggle_config[\"username\"]\n\n# Ensure the dataset ID is slug-friendly\ndataset_title = \"brainTumourSegmentation_model\"\ndataset_slug = dataset_title.replace(\"_\", \"-\").lower()  # Ensure underscores are replaced\ndataset_id = f\"{kaggle_username}/{dataset_slug}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"r\") as f:\n    kaggle_config = json.load(f)\n\nkaggle_username = kaggle_config[\"username\"]\n\ndataset_metadata = {\n    \"title\": dataset_title,\n    \"id\": dataset_id,  # Use the cleaned slug here\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save metadata file\nwith open('/kaggle/working/brainTumourSegmentation_model/dataset-metadata.json', 'w') as f:\n    json.dump(dataset_metadata, f)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/brainTumourSegmentation_model --dir-mode tar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# saving graphs","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport json\n\n# Create target directory for images if it doesn't exist\ntarget_dir = \"/kaggle/working/brainTumourSegmentation_graphs\"\nos.makedirs(target_dir, exist_ok=True)\n\n# Source directory where images are currently located\nsource_dir = \"/kaggle/working/\"\n\n# Move all PNG files from source to target directory\nfor file_name in os.listdir(source_dir):\n    if file_name.endswith(\"graphs.png\"):\n        shutil.move(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n\nprint(f\"All PNG files moved to {target_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Fetch the Kaggle username and API key from environment variables (set earlier)\nkaggle_username = kaggle_config[\"username\"]\nkaggle_key = kaggle_config[\"key\"]\n\n# Ensure that username and key are fetched\nif not kaggle_username or not kaggle_key:\n    raise ValueError(\"KAGGLE_USERNAME and KAGGLE_KEY must be set in environment variables.\")\n\n# Ensure the dataset ID is slug-friendly\ndataset_title = \"brainTumourSegmentation_graphs\"\ndataset_slug = dataset_title.replace(\"_\", \"-\").lower()  # Replace underscores with hyphens\ndataset_id = f\"{kaggle_username}/{dataset_slug}\"\n\n# Create the metadata\ndataset_metadata = {\n    \"title\": dataset_title,\n    \"id\": dataset_id,\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]  # Specify the license for the dataset\n}\n\n# Print the metadata to verify\nprint(json.dumps(dataset_metadata, indent=4))\n\n# Ensure the directory exists before saving the metadata file\nos.makedirs(\"/kaggle/working/brainTumourSegmentation_graphs\", exist_ok=True)\n\n# Save the metadata file\nmetadata_path = '/kaggle/working/brainTumourSegmentation_graphs/dataset-metadata.json'\nwith open(metadata_path, 'w') as f:\n    json.dump(dataset_metadata, f)\n\nprint(f\"Metadata saved to {metadata_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/brainTumourSegmentation_graphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# saving segmentation example images","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport json\n\n# Create target directory for images if it doesn't exist\ntarget_dir = \"/kaggle/working/brainTumourSegmentation_images\"\nos.makedirs(target_dir, exist_ok=True)\n\n# Source directory where images are currently located\nsource_dir = \"/kaggle/working/\"\n\n# Move all PNG files from source to target directory\nfor file_name in os.listdir(source_dir):\n    if file_name.endswith(\".png\"):\n        shutil.move(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n\nprint(f\"All PNG files moved to {target_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Fetch the Kaggle username and API key from environment variables (set earlier)\nkaggle_username = kaggle_config[\"username\"]\nkaggle_key = kaggle_config[\"key\"]\n\n# Ensure that username and key are fetched\nif not kaggle_username or not kaggle_key:\n    raise ValueError(\"KAGGLE_USERNAME and KAGGLE_KEY must be set in environment variables.\")\n\n# Ensure the dataset ID is slug-friendly\ndataset_title = \"brainTumourSegmentation_images\"\ndataset_slug = dataset_title.replace(\"_\", \"-\").lower()  # Replace underscores with hyphens\ndataset_id = f\"{kaggle_username}/{dataset_slug}\"\n\n# Create the metadata\ndataset_metadata = {\n    \"title\": dataset_title,\n    \"id\": dataset_id,\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]  # Specify the license for the dataset\n}\n\n# Print the metadata to verify\nprint(json.dumps(dataset_metadata, indent=4))\n\n# Ensure the directory exists before saving the metadata file\nos.makedirs(\"/kaggle/working/brainTumourSegmentation_images\", exist_ok=True)\n\n# Save the metadata file\nmetadata_path = '/kaggle/working/brainTumourSegmentation_images/dataset-metadata.json'\nwith open(metadata_path, 'w') as f:\n    json.dump(dataset_metadata, f)\n\nprint(f\"Metadata saved to {metadata_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/brainTumourSegmentation_images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}